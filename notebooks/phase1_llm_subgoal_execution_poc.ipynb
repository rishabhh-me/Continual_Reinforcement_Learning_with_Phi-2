{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4n6FRn3HN_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb708ae-eb36-43dc-a7db-3c8e44d6d3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Torch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Deterministic seed set to: 42\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Colab environment sanity check\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "print(\"Deterministic seed set to:\", SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2 FIX: Clean environment + correct packages\n",
        "\n",
        "!pip uninstall -y gym gym-minigrid minigrid >/dev/null 2>&1\n",
        "\n",
        "!pip install -q \\\n",
        "    gymnasium \\\n",
        "    minigrid \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    einops\n"
      ],
      "metadata": {
        "id": "-WsZw33hWq_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceae9f9f-c369-4db0-ba31-0c6f68d8fa93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/136.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2 FIX: Verification\n",
        "\n",
        "import gymnasium as gym\n",
        "import minigrid\n",
        "import transformers\n",
        "import accelerate\n",
        "import einops\n",
        "import torch\n",
        "\n",
        "print(\"gymnasium:\", gym.__version__)\n",
        "print(\"minigrid:\", minigrid.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"accelerate:\", accelerate.__version__)\n",
        "print(\"einops:\", einops.__version__)\n",
        "print(\"torch:\", torch.__version__, \"| cuda:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PanIQzI7W-3Y",
        "outputId": "2635340f-fdf1-4d8e-8bef-d3354f89461b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gymnasium: 1.2.3\n",
            "minigrid: 3.0.0\n",
            "transformers: 4.57.3\n",
            "accelerate: 1.12.0\n",
            "einops: 0.8.1\n",
            "torch: 2.9.0+cu126 | cuda: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Canonical subgoal grammar + deterministic parser\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# ----- Canonical Subgoal Definition -----\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Strict Deterministic Parser -----\n",
        "\n",
        "ALLOWED_SUBGOALS = {\n",
        "    \"GOTO\": 2,     # x y\n",
        "    \"PICK\": 1,     # object_id\n",
        "    \"OPEN\": 1,     # door_id\n",
        "    \"DELIVER\": 3  # x y object_id\n",
        "}\n",
        "\n",
        "def parse_subgoal(text: str) -> Subgoal:\n",
        "    text = text.strip().upper()\n",
        "    tokens = text.split()\n",
        "\n",
        "    if len(tokens) == 0:\n",
        "        raise ValueError(\"Empty subgoal\")\n",
        "\n",
        "    sg_type = tokens[0]\n",
        "\n",
        "    if sg_type not in ALLOWED_SUBGOALS:\n",
        "        raise ValueError(f\"Invalid subgoal type: {sg_type}\")\n",
        "\n",
        "    expected_args = ALLOWED_SUBGOALS[sg_type]\n",
        "    args = tokens[1:]\n",
        "\n",
        "    if len(args) != expected_args:\n",
        "        raise ValueError(\n",
        "            f\"{sg_type} expects {expected_args} args, got {len(args)}\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        args = tuple(int(a) for a in args)\n",
        "    except Exception:\n",
        "        raise ValueError(\"Arguments must be integers\")\n",
        "\n",
        "    return Subgoal(sg_type, args)\n",
        "\n",
        "# ----- Hard Validation Tests -----\n",
        "\n",
        "tests = [\n",
        "    \"GOTO 3 5\",\n",
        "    \"PICK 2\",\n",
        "    \"OPEN 1\",\n",
        "    \"DELIVER 4 7 3\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    sg = parse_subgoal(t)\n",
        "    print(t, \"→\", sg)\n",
        "\n",
        "print(\"STEP 3 PASSED: canonical grammar is deterministic.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLDXhdjnXujx",
        "outputId": "c7086511-260b-490a-bfa9-bf1bc5bede63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOTO 3 5 → Subgoal(type='GOTO', args=(3, 5))\n",
            "PICK 2 → Subgoal(type='PICK', args=(2,))\n",
            "OPEN 1 → Subgoal(type='OPEN', args=(1,))\n",
            "DELIVER 4 7 3 → Subgoal(type='DELIVER', args=(4, 7, 3))\n",
            "STEP 3 PASSED: canonical grammar is deterministic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: MiniGrid environment wrapper + GOTO subgoal success\n",
        "\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# Reuse Subgoal definition from STEP 3\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ---- Environment setup ----\n",
        "\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "env = FullyObsWrapper(env)\n",
        "\n",
        "obs, info = env.reset(seed=42)\n",
        "\n",
        "# ---- Helper: get agent position ----\n",
        "\n",
        "def get_agent_pos(env):\n",
        "    return tuple(env.unwrapped.agent_pos)\n",
        "\n",
        "# ---- Subgoal success check ----\n",
        "\n",
        "def check_subgoal_success(subgoal: Subgoal, env) -> bool:\n",
        "    if subgoal.type == \"GOTO\":\n",
        "        target_x, target_y = subgoal.args\n",
        "        return get_agent_pos(env) == (target_x, target_y)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Only GOTO is supported in STEP 4\")\n",
        "\n",
        "# ---- Test logic ----\n",
        "\n",
        "# Read initial agent position\n",
        "start_pos = get_agent_pos(env)\n",
        "print(\"Initial agent position:\", start_pos)\n",
        "\n",
        "# Define a reachable GOTO subgoal (current position)\n",
        "subgoal = Subgoal(\"GOTO\", start_pos)\n",
        "\n",
        "# Check success\n",
        "success = check_subgoal_success(subgoal, env)\n",
        "print(\"Subgoal:\", subgoal)\n",
        "print(\"Subgoal success:\", success)\n",
        "\n",
        "assert success is True, \"GOTO subgoal should be immediately satisfied\"\n",
        "\n",
        "print(\"STEP 4 PASSED: environment + subgoal success detection works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxgVD9KcaK1Y",
        "outputId": "743b81e3-1566-494f-d119-f57d9782dea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial agent position: (1, 1)\n",
            "Subgoal: Subgoal(type='GOTO', args=(1, 1))\n",
            "Subgoal success: True\n",
            "STEP 4 PASSED: environment + subgoal success detection works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Subgoal-conditioned policy input (NO LEARNING)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# ----- Subgoal definition -----\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Subgoal embedding -----\n",
        "SUBGOAL_TYPES = [\"GOTO\", \"PICK\", \"OPEN\", \"DELIVER\"]\n",
        "SUBGOAL_TO_ID = {k: i for i, k in enumerate(SUBGOAL_TYPES)}\n",
        "\n",
        "class SubgoalEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=16):\n",
        "        super().__init__()\n",
        "        self.type_embedding = nn.Embedding(len(SUBGOAL_TYPES), embed_dim)\n",
        "\n",
        "    def forward(self, subgoal: Subgoal):\n",
        "        sg_type_id = torch.tensor([SUBGOAL_TO_ID[subgoal.type]])\n",
        "        sg_type_emb = self.type_embedding(sg_type_id)\n",
        "        sg_args = torch.tensor(subgoal.args, dtype=torch.float32).unsqueeze(0)\n",
        "        return torch.cat([sg_type_emb, sg_args], dim=1)\n",
        "\n",
        "# ----- Dummy policy network -----\n",
        "class DummyPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim, sg_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim + sg_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, sg_embed):\n",
        "        x = torch.cat([obs, sg_embed], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "# ----- Test forward pass -----\n",
        "\n",
        "# Fake observation (e.g., flattened env state)\n",
        "obs = torch.randn(1, 32)\n",
        "\n",
        "# Example subgoal\n",
        "subgoal = Subgoal(\"GOTO\", (3, 5))\n",
        "\n",
        "sg_encoder = SubgoalEncoder(embed_dim=16)\n",
        "sg_embed = sg_encoder(subgoal)\n",
        "\n",
        "policy = DummyPolicy(\n",
        "    obs_dim=32,\n",
        "    sg_dim=sg_embed.shape[1],\n",
        "    action_dim=7  # MiniGrid action space size\n",
        ")\n",
        "\n",
        "logits = policy(obs, sg_embed)\n",
        "\n",
        "print(\"Observation shape:\", obs.shape)\n",
        "print(\"Subgoal embedding shape:\", sg_embed.shape)\n",
        "print(\"Policy output shape:\", logits.shape)\n",
        "\n",
        "assert logits.shape == (1, 7), \"Policy output shape mismatch\"\n",
        "\n",
        "print(\"STEP 5 PASSED: subgoal-conditioned policy forward pass works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-WAJUyotHiB",
        "outputId": "3b8963fe-c442-49f7-cfd7-f3a1ac245e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation shape: torch.Size([1, 32])\n",
            "Subgoal embedding shape: torch.Size([1, 18])\n",
            "Policy output shape: torch.Size([1, 7])\n",
            "STEP 5 PASSED: subgoal-conditioned policy forward pass works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Executor loop with fixed subgoal (NO LEARNING)\n",
        "\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# ----- Subgoal definition -----\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Environment setup -----\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "env = FullyObsWrapper(env)\n",
        "obs, info = env.reset(seed=42)\n",
        "\n",
        "# ----- Helper: get agent position -----\n",
        "def get_agent_pos(env):\n",
        "    return tuple(env.unwrapped.agent_pos)\n",
        "\n",
        "# ----- Subgoal success check -----\n",
        "def check_subgoal_success(subgoal: Subgoal, env) -> bool:\n",
        "    if subgoal.type == \"GOTO\":\n",
        "        return get_agent_pos(env) == subgoal.args\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# ----- Fixed subgoal (reachable) -----\n",
        "target_pos = get_agent_pos(env)\n",
        "subgoal = Subgoal(\"GOTO\", target_pos)\n",
        "\n",
        "print(\"Initial agent position:\", get_agent_pos(env))\n",
        "print(\"Fixed subgoal:\", subgoal)\n",
        "\n",
        "# ----- Executor loop -----\n",
        "MAX_STEPS = 20\n",
        "done = False\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    # Random action (no policy, no learning)\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    success = check_subgoal_success(subgoal, env)\n",
        "\n",
        "    print(f\"Step {step:02d} | Agent pos: {get_agent_pos(env)} | Subgoal success: {success}\")\n",
        "\n",
        "    if success:\n",
        "        print(\"Subgoal achieved — stopping executor loop.\")\n",
        "        break\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(\"Episode ended by environment.\")\n",
        "        break\n",
        "\n",
        "print(\"STEP 6 PASSED: executor loop with fixed subgoal works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDX2JWwgwDWh",
        "outputId": "6fd3583e-779f-402a-fa10-28b5a54a066f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial agent position: (1, 1)\n",
            "Fixed subgoal: Subgoal(type='GOTO', args=(1, 1))\n",
            "Step 00 | Agent pos: (1, 1) | Subgoal success: True\n",
            "Subgoal achieved — stopping executor loop.\n",
            "STEP 6 PASSED: executor loop with fixed subgoal works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Intrinsic subgoal reward computation (NO LEARNING)\n",
        "\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# ----- Subgoal definition -----\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Environment setup -----\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "env = FullyObsWrapper(env)\n",
        "obs, info = env.reset(seed=42)\n",
        "\n",
        "# ----- Helpers -----\n",
        "def get_agent_pos(env):\n",
        "    return tuple(env.unwrapped.agent_pos)\n",
        "\n",
        "def check_subgoal_success(subgoal: Subgoal, env) -> bool:\n",
        "    if subgoal.type == \"GOTO\":\n",
        "        return get_agent_pos(env) == subgoal.args\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# ----- Intrinsic reward function -----\n",
        "R_COMPLETE = 1.0\n",
        "R_STEP_PENALTY = -0.01\n",
        "\n",
        "def intrinsic_reward(subgoal: Subgoal, env) -> float:\n",
        "    if check_subgoal_success(subgoal, env):\n",
        "        return R_COMPLETE\n",
        "    return R_STEP_PENALTY\n",
        "\n",
        "# ----- Test logic -----\n",
        "target_pos = get_agent_pos(env)\n",
        "subgoal = Subgoal(\"GOTO\", target_pos)\n",
        "\n",
        "print(\"Initial agent position:\", get_agent_pos(env))\n",
        "print(\"Subgoal:\", subgoal)\n",
        "\n",
        "reward = intrinsic_reward(subgoal, env)\n",
        "\n",
        "print(\"Intrinsic reward:\", reward)\n",
        "\n",
        "assert reward == R_COMPLETE, \"Subgoal completion reward incorrect\"\n",
        "\n",
        "print(\"STEP 7 PASSED: intrinsic subgoal reward works correctly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvANf-9bwcsC",
        "outputId": "c2fe608d-d4d6-482d-d536-0ce4db55c489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial agent position: (1, 1)\n",
            "Subgoal: Subgoal(type='GOTO', args=(1, 1))\n",
            "Intrinsic reward: 1.0\n",
            "STEP 7 PASSED: intrinsic subgoal reward works correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Subgoal timeout and failure handling (NO LEARNING)\n",
        "\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# ----- Subgoal definition -----\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Environment setup -----\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "env = FullyObsWrapper(env)\n",
        "obs, info = env.reset(seed=42)\n",
        "\n",
        "# ----- Helpers -----\n",
        "def get_agent_pos(env):\n",
        "    return tuple(env.unwrapped.agent_pos)\n",
        "\n",
        "def check_subgoal_success(subgoal: Subgoal, env) -> bool:\n",
        "    if subgoal.type == \"GOTO\":\n",
        "        return get_agent_pos(env) == subgoal.args\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# ----- Subgoal timeout parameters -----\n",
        "MAX_SUBGOAL_STEPS = 5  # intentionally small\n",
        "R_FAILURE = -0.5\n",
        "\n",
        "# ----- Impossible subgoal (outside grid) -----\n",
        "subgoal = Subgoal(\"GOTO\", (99, 99))\n",
        "\n",
        "print(\"Initial agent position:\", get_agent_pos(env))\n",
        "print(\"Impossible subgoal:\", subgoal)\n",
        "\n",
        "# ----- Executor loop with timeout -----\n",
        "failed = False\n",
        "\n",
        "for step in range(MAX_SUBGOAL_STEPS):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    success = check_subgoal_success(subgoal, env)\n",
        "    print(f\"Step {step:02d} | Agent pos: {get_agent_pos(env)} | Success: {success}\")\n",
        "\n",
        "    if success:\n",
        "        print(\"Unexpected success (should not happen)\")\n",
        "        break\n",
        "\n",
        "else:\n",
        "    failed = True\n",
        "    intrinsic_reward = R_FAILURE\n",
        "    print(\"Subgoal FAILED due to timeout.\")\n",
        "    print(\"Failure intrinsic reward:\", intrinsic_reward)\n",
        "\n",
        "assert failed is True, \"Subgoal failure was not detected\"\n",
        "\n",
        "print(\"STEP 8 PASSED: subgoal timeout and failure handling works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxy-8HRoxUOy",
        "outputId": "bf4d874d-69f7-4089-fa75-5643e3ee4902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial agent position: (1, 1)\n",
            "Impossible subgoal: Subgoal(type='GOTO', args=(99, 99))\n",
            "Step 00 | Agent pos: (1, 1) | Success: False\n",
            "Step 01 | Agent pos: (1, 1) | Success: False\n",
            "Step 02 | Agent pos: (1, 1) | Success: False\n",
            "Step 03 | Agent pos: (1, 1) | Success: False\n",
            "Step 04 | Agent pos: (1, 1) | Success: False\n",
            "Subgoal FAILED due to timeout.\n",
            "Failure intrinsic reward: -0.5\n",
            "STEP 8 PASSED: subgoal timeout and failure handling works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: LLM stub → parser → executor (NO LEARNING)\n",
        "\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "# ----- Subgoal definition -----\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Canonical parser -----\n",
        "ALLOWED_SUBGOALS = {\"GOTO\": 2}\n",
        "\n",
        "def parse_subgoal(text: str) -> Subgoal:\n",
        "    text = text.strip().upper()\n",
        "    tokens = text.split()\n",
        "    if tokens[0] not in ALLOWED_SUBGOALS:\n",
        "        raise ValueError(\"Invalid subgoal\")\n",
        "    if len(tokens[1:]) != ALLOWED_SUBGOALS[tokens[0]]:\n",
        "        raise ValueError(\"Wrong arity\")\n",
        "    return Subgoal(tokens[0], tuple(int(x) for x in tokens[1:]))\n",
        "\n",
        "# ----- LLM STUB -----\n",
        "def llm_stub(env) -> str:\n",
        "    # Always generate a reachable subgoal: current agent position\n",
        "    x, y = env.unwrapped.agent_pos\n",
        "    return f\"GOTO {x} {y}\"\n",
        "\n",
        "# ----- Environment setup -----\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "env = FullyObsWrapper(env)\n",
        "obs, info = env.reset(seed=42)\n",
        "\n",
        "# ----- Helpers -----\n",
        "def get_agent_pos(env):\n",
        "    return tuple(env.unwrapped.agent_pos)\n",
        "\n",
        "def check_subgoal_success(subgoal: Subgoal, env) -> bool:\n",
        "    return get_agent_pos(env) == subgoal.args\n",
        "\n",
        "# ----- Integration loop -----\n",
        "print(\"Initial agent position:\", get_agent_pos(env))\n",
        "\n",
        "raw_subgoal = llm_stub(env)\n",
        "print(\"LLM output:\", raw_subgoal)\n",
        "\n",
        "subgoal = parse_subgoal(raw_subgoal)\n",
        "print(\"Parsed subgoal:\", subgoal)\n",
        "\n",
        "MAX_STEPS = 10\n",
        "success = False\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    success = check_subgoal_success(subgoal, env)\n",
        "    print(f\"Step {step:02d} | Agent pos: {get_agent_pos(env)} | Success: {success}\")\n",
        "\n",
        "    if success:\n",
        "        print(\"Subgoal achieved via LLM stub.\")\n",
        "        break\n",
        "\n",
        "assert success is True, \"LLM → parser → executor chain failed\"\n",
        "\n",
        "print(\"STEP 9 PASSED: full LLM stub integration works.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeObb8P51sWO",
        "outputId": "f3730732-3082-4d1a-86ce-7e306183d298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial agent position: (1, 1)\n",
            "LLM output: GOTO 1 1\n",
            "Parsed subgoal: Subgoal(type='GOTO', args=(1, 1))\n",
            "Step 00 | Agent pos: (1, 1) | Success: True\n",
            "Subgoal achieved via LLM stub.\n",
            "STEP 9 PASSED: full LLM stub integration works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10 FIX: Minimal PPO training with correct obs dimension\n",
        "\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# ----- Subgoal definition -----\n",
        "@dataclass(frozen=True)\n",
        "class Subgoal:\n",
        "    type: str\n",
        "    args: Tuple[int, ...]\n",
        "\n",
        "# ----- Environment -----\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "env = FullyObsWrapper(env)\n",
        "obs, info = env.reset(seed=42)\n",
        "\n",
        "# ----- Helpers -----\n",
        "def get_agent_pos(env):\n",
        "    return tuple(env.unwrapped.agent_pos)\n",
        "\n",
        "def check_subgoal_success(subgoal: Subgoal, env) -> bool:\n",
        "    return get_agent_pos(env) == subgoal.args\n",
        "\n",
        "# ----- LLM stub -----\n",
        "def llm_stub(env):\n",
        "    x, y = env.unwrapped.agent_pos\n",
        "    return Subgoal(\"GOTO\", (x, y))\n",
        "\n",
        "# ----- Determine observation dimension (CRITICAL FIX) -----\n",
        "obs_dim = obs[\"image\"].flatten().shape[0]\n",
        "print(\"Observation dimension:\", obs_dim)\n",
        "\n",
        "# ----- Policy -----\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "policy = Policy(obs_dim, env.action_space.n)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "\n",
        "# ----- Training loop -----\n",
        "EPISODES = 5\n",
        "print(\"Training started...\")\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "    obs, info = env.reset(seed=ep)\n",
        "    subgoal = llm_stub(env)\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for t in range(10):\n",
        "        obs_tensor = torch.tensor(\n",
        "            obs[\"image\"].flatten(),\n",
        "            dtype=torch.float32\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        logits = policy(obs_tensor)\n",
        "        action = torch.distributions.Categorical(logits=logits).sample()\n",
        "\n",
        "        obs, _, terminated, truncated, info = env.step(action.item())\n",
        "\n",
        "        intrinsic = 1.0 if check_subgoal_success(subgoal, env) else -0.01\n",
        "        loss = -logits.mean() * intrinsic\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_reward += intrinsic\n",
        "\n",
        "        if intrinsic > 0:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode {ep} | Total intrinsic reward: {total_reward:.2f}\")\n",
        "\n",
        "print(\"STEP 10 PASSED: PPO-style learning loop executed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dXwnDmK21Zh",
        "outputId": "c46e4208-91cc-4109-8ae1-28364361b4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation dimension: 192\n",
            "Training started...\n",
            "Episode 0 | Total intrinsic reward: 1.00\n",
            "Episode 1 | Total intrinsic reward: 1.00\n",
            "Episode 2 | Total intrinsic reward: 1.00\n",
            "Episode 3 | Total intrinsic reward: 1.00\n",
            "Episode 4 | Total intrinsic reward: 1.00\n",
            "STEP 10 PASSED: PPO-style learning loop executed successfully.\n"
          ]
        }
      ]
    }
  ]
}